# -*- coding: utf-8 -*-
"""DataCleaning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uRcn7MrO8oiFaxLQoj2ybCjHXLYpjb3R
"""

import pandas as pd
import re
import os

# Define the path to the input CSV file
input_file_path = '/content/Lifebear.csv'
chunk_size = 10000  # Adjust the chunk size based on your memory capacity
valid_columns = ['login_id', 'mail_address', 'password', 'salt', 'birthday_on', 'gender']

# Define a regex pattern for valid email addresses and valid login IDs
email_pattern = r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$'
login_id_pattern = r'^[a-zA-Z0-9]+$'  # Allow only alphanumeric characters in login_id

# Initialize counters for naming files and controlling chunk limit
chunk_counter = 1
max_chunks = 4  # Limit to 4 chunks

# Initialize lists to store cleaned chunks and a single garbage chunk
cleaned_chunks = []
garbage_df = pd.DataFrame(columns=valid_columns)  # Single garbage DataFrame

# Function to clean individual chunks
def clean_chunk(chunk):
    global garbage_df  # Use the global garbage DataFrame

    # Select only the valid columns from the chunk
    chunk = chunk[valid_columns].copy()

    # Convert 'birthday_on' to datetime, but leave missing values as NaT
    chunk['birthday_on'] = pd.to_datetime(chunk['birthday_on'], format='%Y-%m-%d', errors='coerce')

    # Identify invalid emails and add to garbage_df
    invalid_emails = ~chunk['mail_address'].apply(lambda x: bool(re.match(email_pattern, str(x))) and x.strip() != "")
    garbage_df = pd.concat([garbage_df, chunk[invalid_emails]], ignore_index=True)
    chunk = chunk[~invalid_emails]  # Keep only valid emails

    # Identify rows with special characters in 'login_id' and add to garbage_df
    invalid_login_ids = ~chunk['login_id'].apply(lambda x: bool(re.match(login_id_pattern, str(x))) and x.strip() != "")
    garbage_df = pd.concat([garbage_df, chunk[invalid_login_ids]], ignore_index=True)
    chunk = chunk[~invalid_login_ids]  # Keep only valid login_ids

    # Drop rows where essential fields like 'login_id' and 'password' are missing and add to garbage_df
    missing_essential = chunk[chunk['login_id'].isna() | chunk['password'].isna()]
    garbage_df = pd.concat([garbage_df, missing_essential], ignore_index=True)
    chunk.dropna(subset=['login_id', 'password'], inplace=True)

    # Handle duplicates
    duplicates = chunk[chunk.duplicated(keep=False)]
    if not duplicates.empty:
        garbage_df = pd.concat([garbage_df, duplicates], ignore_index=True)
    chunk.drop_duplicates(inplace=True)

    # Return the cleaned chunk
    return chunk

# Iterate through the CSV in chunks
for chunk in pd.read_csv(input_file_path, sep=';', low_memory=False, chunksize=chunk_size):
    # Clean the chunk and get the cleaned data
    cleaned_chunk = clean_chunk(chunk)

    # Add the cleaned chunk to the list
    cleaned_chunks.append(cleaned_chunk)

    # Save cleaned chunk to a separate CSV file
    cleaned_file_path = f'/content/cleaned_chunk_{chunk_counter}.csv'
    cleaned_chunk.to_csv(cleaned_file_path, index=False)
    print(f'Cleaned chunk {chunk_counter} saved to {cleaned_file_path}')

    # Print the cleaned chunk
    print(f'Processed chunk {chunk_counter}:')
    print(cleaned_chunk)  # Print the cleaned chunk

    chunk_counter += 1
    # Stop after processing 4 chunks
    if chunk_counter > max_chunks:
        break

# Concatenate all cleaned chunks into one DataFrame
final_cleaned_data = pd.concat(cleaned_chunks, ignore_index=True)

# Save the final garbage data to a single CSV file
if not garbage_df.empty:
    garbage_file_path = '/content/final_garbage_data.csv'
    if not os.path.exists(garbage_file_path):  # Check if the file already exists
        garbage_df.to_csv(garbage_file_path, index=False)
        print('All garbage data has been saved to final_garbage_data.csv')

# Save the merged cleaned data to a final CSV file
cleaned_file_path = '/content/final_cleaned_data.csv'
if not os.path.exists(cleaned_file_path):  # Check if the file already exists
    final_cleaned_data.to_csv(cleaned_file_path, index=False)
    print('All cleaned chunks have been merged into final_cleaned_data.csv')